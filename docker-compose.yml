version: '3'

services:
  # 1. THE DATABASE (Source System)
  mysql:
    image: mysql:8.0
    container_name: mysql_source
    environment:
      MYSQL_ROOT_PASSWORD: rootpassword
      MYSQL_DATABASE: ecommerce_db
      MYSQL_USER: root
      MYSQL_PASSWORD: root 
    ports:
      - "3307:3306" # Maps localhost:3307 to container:3306 (avoids conflict with local mysql)
    volumes:
      - mysql_data:/var/lib/mysql

  # 2. THE STREAMING PLATFORM
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000

  kafka:
    image: public.ecr.aws/bitnami/kafka:3.6
    ports:
      - "9094:9094"
    environment:
      - KAFKA_CFG_NODE_ID=0
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=0@kafka:9093
      - KAFKA_CFG_LISTENERS=INTERNAL://:9092,EXTERNAL://:9094,CONTROLLER://:9093
      - KAFKA_CFG_ADVERTISED_LISTENERS=INTERNAL://kafka:9092,EXTERNAL://localhost:9094
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,CONTROLLER:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=INTERNAL
    volumes:
      - kafka_data:/bitnami/kafka


  # 3. THE PROCESSING ENGINE (Spark)
  spark-master:
    image: public.ecr.aws/bitnami/spark:3.5
    container_name: spark-master
    env_file:
      - .env
    environment:
      - SPARK_MODE=master
      - SPARK_DAEMON_MEMORY=512m
    ports:
      - "8080:8080" # Spark Web UI
      - "7077:7077"
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs

  spark-worker:
    image: public.ecr.aws/bitnami/spark:3.5
    container_name: spark-worker
    depends_on:
      - spark-master
    env_file:
      - .env
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=512m  
      - SPARK_WORKER_CORES=1
    volumes:
      - ./spark_jobs:/opt/bitnami/spark/jobs

  # 4. THE ORCHESTRATOR (Airflow)
  # Note: Airflow requires a lot of setup. For now, we will use the 'standalone' mode for simplicity.
  # airflow:
  #   image: apache/airflow:2.7.1
  #   container_name: airflow
  #   environment:
  #     - AIRFLOW__CORE__LOAD_EXAMPLES=False
  #     - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
  #     - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=sqlite:////opt/airflow/airflow.db
  #     - _AIRFLOW_DB_UPGRADE=true
  #     - _AIRFLOW_WWW_USER_CREATE=true
  #     - _AIRFLOW_WWW_USER_USERNAME=admin
  #     - _AIRFLOW_WWW_USER_PASSWORD=admin
  #   ports:
  #     - "8181:8080"
  #   volumes:
  #     - ./dags:/opt/airflow/dags
  #     - ./scripts:/opt/airflow/scripts
  #     - ./requirements.txt:/opt/airflow/requirements.txt
  #   command: bash -c "pip install -r requirements.txt && airflow standalone"
  #   healthcheck:
  #     test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5


volumes:
  mysql_data:
    driver: local
  kafka_data:       
    driver: local